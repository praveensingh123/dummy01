{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.layers import LSTM\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "import collections\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_check(file_name, train=True):\n",
    "    file = open(file_name,'r')\n",
    "    train_dict = dict()\n",
    "    if train:\n",
    "        label = []\n",
    "        no_of_data = [] # for creating the dictionary, gather all data\n",
    "        train_x = [] #\n",
    "        for line in file:\n",
    "            lx = line.split('\\t')\n",
    "            ly = lx[1].rstrip().split(',')\n",
    "            label.append(lx[0]) # labels appending\n",
    "            train_x.append(ly) # used for numbering array\n",
    "            no_of_data.extend(ly) # used for counting unique numbers\n",
    "        \n",
    "        #for calculating the line length\n",
    "        max_len = max([len(i) for i in train_x])\n",
    "        min_len = min([len(i) for i in train_x])\n",
    "        mean_len = np.mean([len(i) for i in train_x])\n",
    "        std_len = np.std([len(i) for i in train_x])\n",
    "        \n",
    "        #print('list of lengths of sentences: ',[len(i) for i in train_x])\n",
    "        length = [len(i) for i in train_x]\n",
    "        plt.boxplot(length)\n",
    "        print('max sentence length: ', max_len)\n",
    "        print('min sentence length: ', min_len)\n",
    "        print('mean:', mean_len)\n",
    "        print('std:', std_len) \n",
    "        \n",
    "        # for checking the frequency and unique words\n",
    "        di = collections.Counter(no_of_data)\n",
    "        print('unique words:',len(di.keys()))\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "        return di\n",
    "    \n",
    "    else:\n",
    "        label = []\n",
    "        no_of_data = [] # for creating the dictionary, gather all data\n",
    "        train_x = [] #\n",
    "        for line in file:\n",
    "            #lx = line.split('\\t')\n",
    "            ly = line.rstrip().split(',')\n",
    "            #label.append(lx[0]) # labels appending\n",
    "            train_x.append(ly) # used for numbering array\n",
    "            no_of_data.extend(ly) # used for counting unique numbers\n",
    "        \n",
    "        #for calculating the line length\n",
    "        max_len = max([len(i) for i in train_x])\n",
    "        min_len = min([len(i) for i in train_x])\n",
    "        mean_len = np.mean([len(i) for i in train_x])\n",
    "        std_len = np.std([len(i) for i in train_x])\n",
    "        \n",
    "        #print('list of lengths of sentences: ',[len(i) for i in train_x])\n",
    "        print('max sentence length: ', max_len)\n",
    "        print('min sentence length: ', min_len)\n",
    "        print('mean:', mean_len)\n",
    "        print('std:', std_len) \n",
    "        \n",
    "        # for checking the frequency\n",
    "        di = collections.Counter(no_of_data)\n",
    "        print('unique words:',len(di.keys()))\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  100\n",
      "min sentence length:  2\n",
      "mean: 26.2512\n",
      "std: 17.84020455488109\n",
      "unique words: 125\n"
     ]
    }
   ],
   "source": [
    "di_train = data_check('training-data-small.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  102\n",
      "min sentence length:  2\n",
      "mean: 26.50456\n",
      "std: 18.126201455528403\n",
      "unique words: 127\n"
     ]
    }
   ],
   "source": [
    "di_test = data_check('test-data-small.txt', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Z4': 9642, 'Z15': 8314, 'X344420902': 6718, 'Y3143': 4832, 'Y1222': 4451, 'Y1940': 4351, 'Y2817': 4342, 'Y1555': 4193, 'Y2321': 4081, 'Y1522': 3890, 'X123474229': 3798, 'Y2307': 3698, 'Y1478': 3429, 'Y1080': 3379, 'Y2170': 3343, 'Y1132': 3326, 'Y3269': 3301, 'Y1702': 3296, 'X773579': 3234, 'Y1486': 3196, 'Y1705': 3105, 'Y2949': 3095, 'Y2680': 3004, 'Y1725': 2965, 'Y2793': 2954, 'Y3232': 2887, 'Y3021': 2876, 'Y2814': 2758, 'Y2072': 2723, 'Y3145': 2697, 'Y2667': 2663, 'X127436348': 2638, 'Y2640': 2636, 'Y1690': 2633, 'Y2024': 2632, 'Y1468': 2630, 'Y1791': 2609, 'Y3346': 2605, 'Y1353': 2602, 'Y3585': 2566, 'Y1238': 2559, 'Y2267': 2526, 'Y1456': 2506, 'Y977': 2506, 'Y2': 2479, 'Y2257': 2473, 'Y3250': 2466, 'Y2684': 2464, 'X62716661': 2464, 'Y1367': 2431, 'Y2849': 2421, 'Y2641': 2411, 'Y3233': 2409, 'Y1206': 2401, 'Y3141': 2387, 'Y2794': 2382, 'X51769735': 2368, 'X277692318': 2330, 'Y2557': 2305, 'X420128991': 2261, 'X174205309': 2239, 'X62718521': 1949, 'X137608184': 1861, 'X48786947': 1852, 'X56982947': 1837, 'X89474323': 1827, 'X18006172': 1826, 'X236398246': 1824, 'X37875376': 1821, 'X417167762': 1799, 'X40087738': 1762, 'X216091985': 1752, 'X147204623': 1690, 'X302743259': 1596, 'X104923833': 1594, 'X166074297': 1589, 'X442767451': 1573, 'X104964213': 1537, 'X225006700': 1494, 'X44092910': 1469, 'X62716332': 1462, 'X374616379': 1410, 'X551805107': 1374, 'X283926738': 1371, 'X51578397': 1351, 'X564074277': 1322, 'X79805718': 1283, 'X367227997': 1250, 'X773545': 1249, 'X299064116': 1238, 'Z27': 1202, 'X524837224': 1197, 'X117371326': 1186, 'X40087170': 1148, 'Z25': 1142, 'X103413307': 1128, 'X315968790': 1118, 'X127972441': 1116, 'X326209': 1106, 'X315340932': 1103, 'X538427688': 1058, 'X341033099': 1026, 'X41787959': 1011, 'X337793919': 969, 'Z26': 931, 'Z22': 915, 'Z13': 759, 'Z28': 538, 'Z23': 524, 'Z30': 425, 'Z14': 343, 'Z21': 317, 'Z24': 300, 'Z8': 170, 'Z29': 166, 'Z20': 164, 'Z12': 156, 'Z18': 133, 'Z17': 77, 'Z6': 60, 'Z3': 52, 'Z2': 50, 'Z19': 50, 'Z7': 26, 'Z16': 4})\n"
     ]
    }
   ],
   "source": [
    "print(di_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words to keep:  118\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in di_train.items():\n",
    "    if di_train[k] > 100:\n",
    "        count+=1\n",
    "print('top words to keep: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_process(file_name, train=True, top_words=99):\n",
    "    file = open(file_name,'r')\n",
    "    if train:\n",
    "        label = []\n",
    "        no_of_data = [] # for creating the dictionary, gather all data\n",
    "        train_x = [] #\n",
    "        for line in file:\n",
    "            lx = line.split('\\t')\n",
    "            ly = lx[1].rstrip().split(',')\n",
    "            label.append(lx[0]) # labels appending\n",
    "            train_x.append(ly) # used for numbering array\n",
    "            no_of_data.extend(ly) # used for counting unique numbers\n",
    "        \n",
    "        #for calculating the line length\n",
    "        max_len = max([len(i) for i in train_x])\n",
    "        min_len = min([len(i) for i in train_x])\n",
    "        mean_len = np.mean([len(i) for i in train_x])\n",
    "        std_len = np.std([len(i) for i in train_x])\n",
    "\n",
    "        print('max sentence length: ', max_len)\n",
    "        print('min sentence length: ', min_len)\n",
    "        print('mean:', mean_len)\n",
    "        print('std:', std_len)\n",
    "\n",
    "        # creating label array of shape(number,) 1-D row\n",
    "        train_label = np.asarray(label).reshape(-1,)\n",
    "        print('label array shape:', train_label.shape)\n",
    "        \n",
    "        # for checking the frequency\n",
    "        di = collections.Counter(no_of_data)\n",
    "        #print('di', di)\n",
    "        \n",
    "        \n",
    "        ddd = dict(di)\n",
    "        dddd = sorted([[value,key] for (key,value) in ddd.items()], reverse=True)\n",
    "        \n",
    "        # starting most frequent word with 3\n",
    "        for i in range(len(dddd)):\n",
    "            dddd[i][0] = i+3\n",
    "\n",
    "        d = dict(dddd)\n",
    "\n",
    "        res = {v:k for k,v in d.items()}\n",
    "        \n",
    "        # here we are assigning dictionary itens from '3', 0 is for padding, 1 is for start, 2 is for except top_words\n",
    "        for i,j in res.items():\n",
    "            if res[i]>top_words+3:\n",
    "                res[i] = 2\n",
    "        file.close()\n",
    "        \n",
    "        train_data = np.asarray(train_x).reshape(-1,)\n",
    "        for index,i in enumerate(train_data):\n",
    "            for pos,k in enumerate(i):\n",
    "                train_data[index][pos] = res[train_data[index][pos]]#train_dict[train_data[index][pos]]\n",
    "            train_data[index].insert(0,1)\n",
    "            \n",
    "        return (train_data, train_label, res)\n",
    "\n",
    "def data_preprocess_test(filename, label_dictionary):\n",
    "    file = open(filename,'r')\n",
    "    test_x = [] #\n",
    "    for line in file:\n",
    "        #lx = line.split('\\t')\n",
    "        ly = line.rstrip().split(',')\n",
    "        test_x.append(ly) # used for labeling\n",
    "            \n",
    "    max_len = max([len(i) for i in test_x])\n",
    "    min_len = min([len(i) for i in test_x])\n",
    "    mean_len = np.mean([len(i) for i in test_x])\n",
    "    std_len = np.std([len(i) for i in test_x])\n",
    "\n",
    "    print('max sentence length: ',max_len)\n",
    "    print('min sentence length: ', min_len)\n",
    "    print('mean:', mean_len)\n",
    "    print('std:', std_len)\n",
    "\n",
    "    file.close()\n",
    "    test_data = np.asarray(test_x).reshape(-1,)\n",
    "    for index,i in enumerate(test_data):\n",
    "        for pos,k in enumerate(i):\n",
    "            test_data[index][pos] = label_dictionary.get(test_data[index][pos],0) \n",
    "        test_data[index] = [j for j in i if j!=0]\n",
    "        test_data[index].insert(0,1)   \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  100\n",
      "min sentence length:  2\n",
      "mean: 26.2512\n",
      "std: 17.84020455488109\n",
      "label array shape: (10000,)\n",
      "[list([1, 21, 35, 31, 3, 4])\n",
      " list([1, 78, 13, 75, 87, 70, 60, 34, 85, 5, 66, 92, 59, 67, 65, 86, 82, 99, 72, 14, 19, 11, 6, 9, 44, 25, 48, 58, 27, 29, 28, 3, 4])\n",
      " list([1, 84, 21, 5, 8, 23, 3, 4, 107]) ...\n",
      " list([1, 5, 19, 18, 30, 44, 48, 58, 27, 49, 55, 28, 16, 10, 7, 31, 43, 32, 57, 6, 39, 20, 25, 23, 50, 33, 26, 54, 35, 17, 56, 42, 36, 52, 41, 24, 29, 61, 14, 53, 15, 11, 22, 37, 12, 9, 40, 46, 8, 45, 38, 3, 4, 97])\n",
      " list([1, 105, 71, 5, 73, 58, 53, 3, 4, 110])\n",
      " list([1, 13, 87, 62, 81, 63, 60, 59, 102, 105, 75, 5, 73, 84, 103, 83, 47, 18, 30, 55, 16, 10, 7, 43, 32, 6, 20, 25, 56, 52, 41, 12, 9, 8, 3, 4])]\n"
     ]
    }
   ],
   "source": [
    "data, target, dictionary = data_process('training-data-small.txt', top_words=118)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  102\n",
      "min sentence length:  2\n",
      "mean: 26.50456\n",
      "std: 18.126201455528403\n"
     ]
    }
   ],
   "source": [
    "test_data = data_preprocess_test('test-data-small.txt', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([1, 90, 98, 59, 44, 48, 31, 17, 11, 53, 3, 4, 97])\n",
      " list([1, 7, 56, 20, 3, 4, 97])\n",
      " list([1, 87, 21, 91, 5, 73, 81, 18, 7, 43, 3, 4, 97]) ...\n",
      " list([1, 64, 63, 86, 51, 5, 103, 21, 96, 106, 83, 74, 73, 81, 69, 13, 95, 60, 90, 98, 48, 49, 55, 7, 6, 61, 11, 38, 10, 20, 23, 37, 9, 45, 3, 4])\n",
      " list([1, 62, 63, 66, 73, 81, 103, 28, 32, 30, 33, 53, 3, 113])\n",
      " list([1, 62, 5, 73, 60, 63, 96, 83, 14, 30, 2, 109])]\n"
     ]
    }
   ],
   "source": [
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 45 # maximum sequence length\n",
    "data = sequence.pad_sequences(data, maxlen=max_len)\n",
    "test_data = sequence.pad_sequences(test_data, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 45) (1000, 45)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 45, 32)            3904      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1440)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               360250    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 364,405\n",
      "Trainable params: 364,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "top_words = 118+4 # we created the words with '0' to '118+3'\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "\n",
    "#model.add(LSTM(128))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      " - 2s - loss: 0.5545 - acc: 0.7153 - val_loss: 0.4890 - val_acc: 0.7500\n",
      "Epoch 2/20\n",
      " - 2s - loss: 0.4771 - acc: 0.7722 - val_loss: 0.4922 - val_acc: 0.7550\n",
      "Epoch 3/20\n",
      " - 2s - loss: 0.4411 - acc: 0.7926 - val_loss: 0.5026 - val_acc: 0.7490\n",
      "Epoch 4/20\n",
      " - 2s - loss: 0.4019 - acc: 0.8166 - val_loss: 0.5243 - val_acc: 0.7330\n",
      "Epoch 5/20\n",
      " - 2s - loss: 0.3477 - acc: 0.8416 - val_loss: 0.5858 - val_acc: 0.7090\n",
      "Epoch 6/20\n",
      " - 2s - loss: 0.2895 - acc: 0.8733 - val_loss: 0.6047 - val_acc: 0.7230\n",
      "Epoch 7/20\n",
      " - 2s - loss: 0.2327 - acc: 0.9056 - val_loss: 0.6843 - val_acc: 0.7140\n",
      "Epoch 8/20\n",
      " - 2s - loss: 0.1874 - acc: 0.9259 - val_loss: 0.7428 - val_acc: 0.7210\n",
      "Epoch 9/20\n",
      " - 2s - loss: 0.1513 - acc: 0.9420 - val_loss: 0.8307 - val_acc: 0.7210\n",
      "Epoch 10/20\n",
      " - 2s - loss: 0.1238 - acc: 0.9560 - val_loss: 0.8993 - val_acc: 0.7170\n",
      "Epoch 11/20\n",
      " - 2s - loss: 0.1011 - acc: 0.9633 - val_loss: 1.0221 - val_acc: 0.6980\n",
      "Epoch 12/20\n",
      " - 2s - loss: 0.0840 - acc: 0.9706 - val_loss: 1.0947 - val_acc: 0.7110\n",
      "Epoch 13/20\n",
      " - 2s - loss: 0.0714 - acc: 0.9762 - val_loss: 1.1437 - val_acc: 0.7090\n",
      "Epoch 14/20\n",
      " - 2s - loss: 0.0590 - acc: 0.9818 - val_loss: 1.2484 - val_acc: 0.7010\n",
      "Epoch 15/20\n",
      " - 2s - loss: 0.0509 - acc: 0.9833 - val_loss: 1.2983 - val_acc: 0.7070\n",
      "Epoch 16/20\n",
      " - 2s - loss: 0.0448 - acc: 0.9849 - val_loss: 1.3914 - val_acc: 0.7070\n",
      "Epoch 17/20\n",
      " - 2s - loss: 0.0421 - acc: 0.9856 - val_loss: 1.5089 - val_acc: 0.6880\n",
      "Epoch 18/20\n",
      " - 2s - loss: 0.0412 - acc: 0.9861 - val_loss: 1.4725 - val_acc: 0.7130\n",
      "Epoch 19/20\n",
      " - 2s - loss: 0.0375 - acc: 0.9873 - val_loss: 1.5471 - val_acc: 0.7100\n",
      "Epoch 20/20\n",
      " - 2s - loss: 0.0325 - acc: 0.9884 - val_loss: 1.6022 - val_acc: 0.7110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16449e80748>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.0315 - acc: 0.9888 - val_loss: 1.6195 - val_acc: 0.7030\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0310 - acc: 0.9894 - val_loss: 1.6975 - val_acc: 0.7050\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0273 - acc: 0.9889 - val_loss: 1.7493 - val_acc: 0.7010\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0261 - acc: 0.9896 - val_loss: 1.7979 - val_acc: 0.6980\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0246 - acc: 0.9908 - val_loss: 1.8333 - val_acc: 0.7000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0771 - acc: 0.9769 - val_loss: 1.7950 - val_acc: 0.7090\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0535 - acc: 0.9822 - val_loss: 1.7099 - val_acc: 0.6950\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0273 - acc: 0.9903 - val_loss: 1.7303 - val_acc: 0.6890\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0225 - acc: 0.9916 - val_loss: 1.8074 - val_acc: 0.6940\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.0208 - acc: 0.9907 - val_loss: 1.7917 - val_acc: 0.7010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16449e80908>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.0215 - acc: 0.9909 - val_loss: 1.8377 - val_acc: 0.6970\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0203 - acc: 0.9914 - val_loss: 1.8379 - val_acc: 0.6920\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0189 - acc: 0.9919 - val_loss: 1.8692 - val_acc: 0.7040\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0197 - acc: 0.9911 - val_loss: 1.8877 - val_acc: 0.6940\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0192 - acc: 0.9910 - val_loss: 1.9518 - val_acc: 0.6990\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0203 - acc: 0.9914 - val_loss: 1.9074 - val_acc: 0.6940\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0191 - acc: 0.9910 - val_loss: 1.9683 - val_acc: 0.6930\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0201 - acc: 0.9908 - val_loss: 2.0653 - val_acc: 0.6930\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0267 - acc: 0.9898 - val_loss: 1.9942 - val_acc: 0.6920\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.0457 - acc: 0.9848 - val_loss: 1.9034 - val_acc: 0.6900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16440f1eef0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_output = model.predict(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(test_output.shape[0]):\n",
    "    if test_output[i] > 0.5:\n",
    "        test_output[i] = 1.0\n",
    "    else:\n",
    "        test_output[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('test_small.txt', test_output, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = np.loadtxt('test_small.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open (\"test.txt\",\"w\")as fp:\n",
    "   for line in list12:\n",
    "       fp.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
