{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thakur/newenv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LSTM\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = dict()\n",
    "\n",
    "global train_dict\n",
    "\n",
    "def data_process(file_name, train=True):\n",
    "    file = open(file_name,'r')\n",
    "    if train:\n",
    "        label = []\n",
    "        no_of_data = [] # for creating the dictionary, gather all data\n",
    "        train_x = [] #\n",
    "        for line in file:\n",
    "            lx = line.split('\\t')\n",
    "            ly = lx[1].rstrip().split(',')\n",
    "            label.append(lx[0]) # labels appending\n",
    "            train_x.append(ly) # used for numbering array\n",
    "            no_of_data.extend(ly) # used for counting unique numbers\n",
    "        \n",
    "        #for calculating the line length\n",
    "        max_len = max([len(i) for i in train_x])\n",
    "        min_len = min([len(i) for i in train_x])\n",
    "        mean_len = np.mean([len(i) for i in train_x])\n",
    "        std_len = np.std([len(i) for i in train_x])\n",
    "\n",
    "        print('max sentence length: ', max_len)\n",
    "        print('min sentence length: ', min_len)\n",
    "        print('mean:', mean_len)\n",
    "        print('std:', std_len)\n",
    "\n",
    "        # creating label array of shape(number,) 1-D row\n",
    "        train_label = np.asarray(label).reshape(-1,)\n",
    "        print('label array shape:', train_label.shape)\n",
    "        \n",
    "        # for checking the frequency\n",
    "        di = collections.Counter(no_of_data)\n",
    "        print('di', di)\n",
    "        \n",
    "        \n",
    "        ddd = dict(di)\n",
    "        dddd = sorted([[value,key] for (key,value) in ddd.items()], reverse=True)\n",
    "\n",
    "        for i in range(len(dddd)):\n",
    "            dddd[i][0] = i+2\n",
    "\n",
    "        d = dict(dddd)\n",
    "\n",
    "        res = {v:k for k,v in d.items()}\n",
    "        \n",
    "        # so we create if the number is greater than 99 as '1'. So our vocabulary has '1' to '99'.\n",
    "        # by adding '0', vocabulary has '0' to '99'\n",
    "        for i,j in res.items():\n",
    "            if res[i]>99:\n",
    "                res[i] = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        # here you can create unique keys\n",
    "        for i,j in enumerate(no_of_data):\n",
    "            train_dict[j] = i\n",
    "\n",
    "        print('unique words:',len(train_dict.keys()))\n",
    "\n",
    "        # making the dictionary start with (one) zero (+1, = start with '1') not in the or\n",
    "        #for i,j in enumerate(train_dict.keys()):\n",
    "        #    train_dict[j] = i+1\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        train_data = np.asarray(train_x).reshape(-1,)\n",
    "        for index,i in enumerate(train_data):\n",
    "            for pos,k in enumerate(i):\n",
    "                train_data[index][pos] = res[train_data[index][pos]]#train_dict[train_data[index][pos]]\n",
    "\n",
    "        return (train_data, train_label, di)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        test_x = [] #\n",
    "        for line in file:\n",
    "            #lx = line.split('\\t')\n",
    "            ly = line.rstrip().split(',')\n",
    "            test_x.append(ly) # used for labeling\n",
    "            \n",
    "        max_len = max([len(i) for i in test_x])\n",
    "        min_len = min([len(i) for i in test_x])\n",
    "        mean_len = np.mean([len(i) for i in test_x])\n",
    "        std_len = np.std([len(i) for i in test_x])\n",
    "\n",
    "        print('max sentence length: ',max_len)\n",
    "        print('min sentence length: ', min_len)\n",
    "        print('mean:', mean_len)\n",
    "        print('std:', std_len)\n",
    "\n",
    "        file.close()\n",
    "        test_data = np.asarray(test_x).reshape(-1,)\n",
    "        for index,i in enumerate(test_data):\n",
    "            for pos,k in enumerate(i):\n",
    "                test_data[index][pos] = train_dict[test_data[index][pos]]\n",
    "                \n",
    "        return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  100\n",
      "min sentence length:  2\n",
      "mean: 26.2512\n",
      "std: 17.84020455488109\n",
      "label array shape: (10000,)\n",
      "di Counter({'Z4': 9642, 'Z15': 8314, 'X344420902': 6718, 'Y3143': 4832, 'Y1222': 4451, 'Y1940': 4351, 'Y2817': 4342, 'Y1555': 4193, 'Y2321': 4081, 'Y1522': 3890, 'X123474229': 3798, 'Y2307': 3698, 'Y1478': 3429, 'Y1080': 3379, 'Y2170': 3343, 'Y1132': 3326, 'Y3269': 3301, 'Y1702': 3296, 'X773579': 3234, 'Y1486': 3196, 'Y1705': 3105, 'Y2949': 3095, 'Y2680': 3004, 'Y1725': 2965, 'Y2793': 2954, 'Y3232': 2887, 'Y3021': 2876, 'Y2814': 2758, 'Y2072': 2723, 'Y3145': 2697, 'Y2667': 2663, 'X127436348': 2638, 'Y2640': 2636, 'Y1690': 2633, 'Y2024': 2632, 'Y1468': 2630, 'Y1791': 2609, 'Y3346': 2605, 'Y1353': 2602, 'Y3585': 2566, 'Y1238': 2559, 'Y2267': 2526, 'Y1456': 2506, 'Y977': 2506, 'Y2': 2479, 'Y2257': 2473, 'Y3250': 2466, 'Y2684': 2464, 'X62716661': 2464, 'Y1367': 2431, 'Y2849': 2421, 'Y2641': 2411, 'Y3233': 2409, 'Y1206': 2401, 'Y3141': 2387, 'Y2794': 2382, 'X51769735': 2368, 'X277692318': 2330, 'Y2557': 2305, 'X420128991': 2261, 'X174205309': 2239, 'X62718521': 1949, 'X137608184': 1861, 'X48786947': 1852, 'X56982947': 1837, 'X89474323': 1827, 'X18006172': 1826, 'X236398246': 1824, 'X37875376': 1821, 'X417167762': 1799, 'X40087738': 1762, 'X216091985': 1752, 'X147204623': 1690, 'X302743259': 1596, 'X104923833': 1594, 'X166074297': 1589, 'X442767451': 1573, 'X104964213': 1537, 'X225006700': 1494, 'X44092910': 1469, 'X62716332': 1462, 'X374616379': 1410, 'X551805107': 1374, 'X283926738': 1371, 'X51578397': 1351, 'X564074277': 1322, 'X79805718': 1283, 'X367227997': 1250, 'X773545': 1249, 'X299064116': 1238, 'Z27': 1202, 'X524837224': 1197, 'X117371326': 1186, 'X40087170': 1148, 'Z25': 1142, 'X103413307': 1128, 'X315968790': 1118, 'X127972441': 1116, 'X326209': 1106, 'X315340932': 1103, 'X538427688': 1058, 'X341033099': 1026, 'X41787959': 1011, 'X337793919': 969, 'Z26': 931, 'Z22': 915, 'Z13': 759, 'Z28': 538, 'Z23': 524, 'Z30': 425, 'Z14': 343, 'Z21': 317, 'Z24': 300, 'Z8': 170, 'Z29': 166, 'Z20': 164, 'Z12': 156, 'Z18': 133, 'Z17': 77, 'Z6': 60, 'Z3': 52, 'Z2': 50, 'Z19': 50, 'Z7': 26, 'Z16': 4})\n",
      "unique words: 125\n"
     ]
    }
   ],
   "source": [
    "data, target, dii = data_process('training-data-small.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([20, 34, 30, 2, 3])\n",
      " list([77, 12, 74, 86, 69, 59, 33, 84, 4, 65, 91, 58, 66, 64, 85, 81, 98, 71, 13, 18, 10, 5, 8, 43, 24, 47, 57, 26, 28, 27, 2, 3])\n",
      " list([83, 20, 4, 7, 22, 2, 3, 1]) ...\n",
      " list([4, 18, 17, 29, 43, 47, 57, 26, 48, 54, 27, 15, 9, 6, 30, 42, 31, 56, 5, 38, 19, 24, 22, 49, 32, 25, 53, 34, 16, 55, 41, 35, 51, 40, 23, 28, 60, 13, 52, 14, 10, 21, 36, 11, 8, 39, 45, 7, 44, 37, 2, 3, 96])\n",
      " list([1, 70, 4, 72, 57, 52, 2, 3, 1])\n",
      " list([12, 86, 61, 80, 62, 59, 58, 1, 1, 74, 4, 72, 83, 1, 82, 46, 17, 29, 54, 15, 9, 6, 42, 31, 5, 19, 24, 55, 51, 40, 11, 8, 7, 2, 3])]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '1' ... '1' '1' '0']\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ddd = dict(dii)\n",
    "dddd = sorted([[value,key] for (key,value) in ddd.items()], reverse=True)\n",
    "        \n",
    "for i in range(len(dddd)):\n",
    "    dddd[i][0] = i+2\n",
    "        \n",
    "d = dict(dddd)\n",
    "        \n",
    "res = {v:k for k,v in d.items()}\n",
    "        \n",
    "for i,j in res.items():\n",
    "    if res[i]>99:\n",
    "        res[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 50 # maximum sequence length\n",
    "data = sequence.pad_sequences(data, maxlen=max_words)\n",
    "#X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "top_words = 100\n",
    "max_words = 50\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 64, input_length=max_words))\n",
    "#model.add(Flatten())\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, verbose=2)\n",
    "# Final evaluation of the model\n",
    "#scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "#print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
